\documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{fancyvrb,relsize}



\title{Parallelizing Automatic Identification of Word Translations from Unrelated Corpora}
\author{Leah Hanson, Matthias Lee, Mike Weinberger}
\begin{document}
\maketitle
\begin{abstract}

Implementation of a machine translation technique which allows for learning word translations from unrelated corpora. We offer a methods for scaling this technique to highly parallel graphics processors, using Python, NLTK and PyCUDA on NVIDA GPUs. We aim to highlight the advantages of using GPUs to speedup the vector calculation necessary for relating source words to their foriegn counterparts.

\end{abstract}
\section{Introduction}
% Describes problem & proposed solution

The exponential growth of technologies such as the Internet and Social media, it has become easier than ever to harvest enormus monolingual dataset. There has been extensive research in the field of machine translation for parallel corpora, but little attention has been paid to learning from non-parallel monolingual corpora. For the most common of languages, parallel corpora are easy to come by, but for lesser known languages it may be hard to find a large enough parallel text. Though languages rarely lack in monolingual texts.

We based our approach on \cite{rapp1999automatic} and \cite{rapp1995identifying}, which soley relies on the notion that words tend to co-occur in similar frequency across different languages. Rapp cites that even very different languages such as Chinese and English share these co-occurences. We firstly aimed to replecate their results in a serial manner as a baseline and then take a step further to rework their vector calculation approach to allow for porting to graphics processors with PyCUDA.

Rapp's approach is based on building co-occurence matrix, which keeps track of co-occurences between words appearing within a range $w$ from on another. These matricies are computed for both corpora and are then transformed into association matricies, by calculating the log-likelihood ratio for each co-occurence. These association matricies/vectors can then be compared with a simple Euclidian distance metric such as the city-block metric.

Many of these calculations are embarrassingly parallel in nature and are therefore great candidates for GPU processing.


\section{Approach}
% Basic Outline
% Languages: French, German, Spanish
% Data Sources (Copuses, Base Lexicon, Test Words)
% Preprocessing
% Build Vectors
% Run tests

To resolve a basic connection between our source and foreign corpora we start with a base dictionary for each language pair. In this project we are aiming to expand dictionaries for English-Spanish, English-French, and English-
German. Chris Callison-Burch provided us with our base dictionaries. We preprocessed each of them to strip all multi-word translations and to only include one translation for each english word. Next we needed to appropriate four large corpora. This was done by mirroring Project Gutenbergs collection for each of the languages.

\subsection{Preprocessing}

For the dictionaries, as mentioned above, removed all multi-word phrases, one-to-many and many-to-one translations to give us a simple and uniform dictionary. The caveat here is that by only keeping one translation from the one-to-many entries, we may not have choosen the best translations. Each of the dictionaries was then split it into three parts. Most of each dictionary went into a base lexicon, roughly 20000 entries per language. Next we split off roughly 5000 entries for one test set and one development set.

To create each corpora we had to preprocess the Project Gutenberg books to strip out License agreements, notes, dictionaries and unreadable characters. For each corpora, we removed some common function words -- pronouns, articles,
and prepositions, then segmented the corpora into sentences and words using the Python NLTK's default splitters. Next we stemmed all the words in each corpora and dictionaries using the Python NLTK's SnowballStemmer (for the appropriate
language).

After preprocessing each corpus and dictionary, we constructed co-occurence vectors which we then cleaned and trimmed to remove noise and save memory. Each of these was then transformed into an association vector. Once we had association vectors to represent the words in each corpus, we were ready to attempt the translations.

%TODO: did we remove words that occured fewer than 100 times? at what stage?

From the foreign language corpus, we pulled out the association vectors representing test words from our test dictionary. Since each of those vectors is already described in terms of words of the base lexicon, we used the base lexicon to translate each ``association word'' into English, so that the vectors would be in the same space as their English counter parts.

Next we can calculate the distanc between each foreign word vector and each English(source) word vector by calculating a simple Euclidian distance metric, the City-Block distance, the sum of each absolute element wise distance. For each foreign language word, the English word with the shortest City-Block distance is chosen as the best translation.

\subsection{Co-occurrence Counting}

The goal is to end up with a set of vectors representing a the co-occurences between all words with in our corpus. Each vector
represents the context that one word (that appeared in the corpus) appeared in. This can be though of as counting how many times word A appears within some given distance $w$ from word B. Based on Rapp's research we chose $w=3$, meaning for every word B we count all words, from word B's position -3 to +3. These vectors are composed of \{word B : (word A,number)\} mappings. Each mapping represents the number of times a certain word A has co-occured around word B within the range $w$. As suggested by \cite{rapp1999automatic}, we also kept track of the distance between word A and word B in our mappings. We handled this by prefixing the mapping of each word A with its original position in context. This better captures the relation between the co-occurring words. For each of the six position within our $w$-sized window, we first generate a separate vector and then collapse them into one to creat a vector of size six. For example, one of the six vectors would represent the words that
occur two words before the target word. In practice, we just prepended each word with a number to represent which position it was found in, so the keys of the dictionary differentiate between the positions.

\subsection{Association Vector}
% take a context vector
% prune to base lexicon
% convert values
% normalize

Turning a co-occurrence vector into an association vector is fairly straight-forward. For each vector, we remove all words that are not in out base lexicon. We do this to endup with a vector that can be fully described in terms of the relations provided by the base lexicon. At this point, we apply an optimized log-likelihood ration formula presented in Rapp's original paper. (\emph{ see equation \ref{loglike}}). After we have calculated our association vectors, we normalize each vector to one.

To convert a raw co-occurrence count into an association value, you need: the count, the total number of times the word this context vector represents was seen, the number of times this word was seen, and the total number of word tokens in the corpus.

\begin{figure}
$k_{11} =$ frequency of common occurrence of word A and word B \\
$k_{12} =$ corpus frequency of word $A - k_{11}$ \\
$k_{21} =$ corpus frequency of word $B - k_{11}$ \\
$k_{22} =$ size of corpus (in tokens) - corpus freq. of A - corpus freq. of B \\

$C_1$ = $k_{11} + k_{12}$
$C_2$ = $k_{21} + k_{22}$ \\
$R_1$ = $k_{11} + k_{21}$
$R_2$ = $k_{12} + k_{22}$ \\
$N$ = $k_{11} + k_{12} + k_{21} + k_{22}$ \\

  \begin{eqnarray*}
\sum_{i,j\in\{1,2\}} k_{ij}\log\frac{k_{ij}N}{C_iR_j} &= k_{11}\log\frac{k_{11}N}{C_1R_1} +  k_{12}\log\frac{k_{12}N}{C_1R_2} + \\
  & k_{21}\log\frac{k_{21}N}{C_2R_1} +  k_{22}\log\frac{k_{22}N}{C_2R_2} \\
  \end{eqnarray*}

  \caption{Optimized log-likelihood ration formula.}
  \label{loglike}
\end{figure}


\subsection{Vector Similarity}

Now that we have association vectors, we can calculate the similarity between two vectors by simply calculating a Euclidian ``City-Block" distance, which is the sum of each absolute element-wise distance. (\emph{ see equation \ref{cityblock}}) To find the best translation between one of our test vectors and our English source vectors, we minimize on the distance between the vector pairs. Therefore we find the closest matching vector from the other language, this is our best match and we consider it a translation.

\begin{figure}
$$\sum_{i\in U,V} |U_i - V_i| $$
\caption{City-Block distance}
  \label{cityblock}
\end{figure}

\section{Parallelizing Vector Calculations}

The parallelization effort of this project was mainly fullfilled my removing our complicated Python dictionary structers and placing our data into flat, extremely large array which could easily be mapped to GPU memory. The main part that was outsourced to the GPU was the association vector calculation. We begin by transforming our input vectors into a 1 dimensional array which the GPU will access based on a stride(width of 1 set of variables). The actual communication with CUDA was done through the Python wrapper class called PyCUDA, which automatically compiles and interfaces with the GPU's C-based libraries.

The GPU is then set to launch as many threads as there are co-occurence values to be calculated. This may sound inefficient, but GPU threads are extremely light weight with very little overhead. Depending on the size of the vectors being calculated, we launch close to 3.5 Million threads at once, arranged in block of 512 threads. Each block is mapped to 1 multiprocessor. Depending on the graphics card used to run this code, it may have between 2 and 32 of these multi processors, each capable of concurrently executing between 16 and 32 threads(depending on architecture).

\section{Results}

Since our results are mainly focused on speedup gained with the same amount of processing we will mostly discuss the performance for the same operations CPU vs GPU. The GPU executing a small test on a laptop, transforming an english and german co-occurence matrix, took 0.0004839seconds and 0.000082015seconds respectively. The counter part on the CPU too 0.636 seconds and 1.3375 seconds respectively. This however does not cover the full overhead of what the GPU needs to do with setup. Including this extra cost of setting up memory and copying between CPU and GPU memory, we arrive at more comparable numbers of 0.47 and 0.948 seconds respectively.

Small laptop sized tests such as mentioned above do not to the capabillity of the GPU justice. So we reran the same test, scaled up in size by about <waiting for thing to finish so i can put stuff here>

\section{Conclusions}
As prooven above, we can see that the GPU out performs the CPU implementation by far. But we also need to consider that we are comparing a single code CPU application against a highly parallel GPU application. In reality, the CPU implementation, if translated into C or C++ and made multi-threaded it would be a much closer competition, but overall the GPU will still take the lead and leave even the fastest CPUs in the dust.

\bibliography{paper}
\bibliographystyle{plain}
\end{document}