\documentclass[12pt]{article}

\title{Parallelizing Automatic Identification of Word Translations from Unrelated Corpora}
\author{Leah Hanson, Matthias Lee, Mike Weinberger}
\begin{document}
\maketitle
\begin{abstract}

Implementation of a machine translation technique which allows for learning word translations from unrelated corpora. We offer a methods for scaling this technique to highly parallel graphics processors, using Python, NLTK and PyCUDA on NVIDA GPUs. We aim to highlight the advantages of using GPUs to speedup the vector calculation necessary for relating source words to their foriegn counterparts.

\end{abstract}
\section{Introduction}
% Describes problem & proposed solution

The exponential growth of technologies such as the Internet and Social media, it has become easier than ever to harvest enormus monolingual dataset. There has been extensive research in the field of machine translation for parallel corpora, but little attention has been paid to learning from non-parallel monolingual corpora. For the most common of languages, parallel corpora are easy to come by, but for lesser known languages it may be hard to find a large enough parallel text. Though languages rarely lack in monolingual texts. 

We based our approach on \cite{rapp1999automatic} and \cite{rapp1995identifying}, which soley relies on the notion that words tend to co-occur in similar frequency across different languages. Rapp cites that even very different languages such as Chinese and English share these co-occurences. We firstly aimed to replecate their results in a serial manner as a baseline and then take a step further to rework their vector calculation approach to allow for porting to graphics processors with PyCUDA. 

Rapp's approach is based on building co-occurence matrix, which keeps track of co-occurences between words appearing within a range $w$ from on another. These matricies are computed for both corpora and are then transformed into association matricies, by calculating the log-likelihood ratio for each co-occurence. These association matricies/vectors can then be compared with a simple Euclidian distance metric such as the city-block metric.

Many of these calculations are embarrassingly parallel in nature and are therefore great candidates for GPU processing.


\section{Approach}
% Basic Outline
% Languages: French, German, Spanish
% Data Sources (Copuses, Base Lexicon, Test Words)
% Preprocessing
% Build Vectors
% Run tests

To resolve a basic connection between our source and foreign corpora we start with a base dictionary for each language pair. In this project we are aiming to expand dictionaries for English-Spanish, English-French, and English-
German. Chris Callison-Burch provided us with our base dictionaries. We preprocessed each of them to strip all multi-word translations and to only include one translation for each english word. Next we needed to appropriate four large corpora. This was done by mirroring Project Gutenbergs collection for each of the languages.

\subsection{Preprocessing}

For the dictionaries, as mentioned above, removed all multi-word phrases, one-to-many and many-to-one translations to give us a simple and uniform dictionary. The caveat here is that by only keeping one translation from the one-to-many entries, we may not have choosen the best translations. Each of the dictionaries was then split it into three parts. Most of each dictionary went into a base lexicon, roughly 20000 entries per language. Next we split off roughly 5000 entries for one test set and one development set.

To create each corpora we had to preprocess the Project Gutenberg books to strip out License agreements, notes, dictionaries and unreadable characters. For each corpora, we removed some common function words -- pronouns, articles,
and prepositions, then segmented the corpora into sentences and words using the Python NLTK's default splitters. Next we stemmed all the words in each corpora and dictionaries using the Python NLTK's SnowballStemmer (for the appropriate
language).

After preprocessing each corpus, we built co-occurence vectors. Each of these was then transformed into an association vector. Once we had association vectors to represent the words in each corpus, we were ready to attempt the translations.

%TODO: did we remove words that occured fewer than 100 times? at what stage?

From the target language corpora, we pulled out the association vectors
representing test words. Since each of those vectors is already in terms of
words in the base lexicon, we used the base lexicon to translate each word into
English, so that the vectors would be in the same space as their English counter
parts.

Each target langauge vector then gets compared with each English word vector,
using the City Block Distance as the comparision. For each target language word,
the English word with the closest association vector is chosen as the
translation.

\subsection{Co-occurrence Counting}

The goal is to end up with a set of vectors representing a corpus. Each vector
represents the context that one word (that appeared in the corpus) appeared in.
The vector is composed of (word,number) mappings. Each mapping represents the
number of times the word being mapped occurs in a window around the word the
vector is describing.

We used a window size of three, so each word in the
vector appeared in the three words before or the three words after the word
being described.

Rather than using the default of just putting all co-occurrences within the
window in one vector, we made position within the matter. To do this, the
final vector is created from six separate vectors: one for each position in
the window. For example, one of the six vectors would represent the words that
occur two words before the target word. In practice, we just prepended each
word with a number to represent which position it was found in, so the keys of
the dictionary differentiate between the positions.

\subsection{Association Vector}
% take a context vector
% prune to base lexicon
% convert values
% normalize

Turning a co-occurrence vector into an association vector is actually fairly
straight-forward. For each vector, remove all words that are not in the base
lexicon. At this point, use a fancy formula to convert each raw count into an
association value.  Then, normalize the values so that each vector sums to
one.

To convert a raw co-occurrence count into an association value, you need: the
count, the total number of times the word this context vector represents was
seen, the number of times this word was seen, and the total number of word
tokens in the corpus.

\begin{description}
\item[$k_{11}$ =] frequency of common occurrence of word A and word B
\item[$k_{12}$ =] corpus frequency of word A $- k_{11}$
\item[$k_{21}$ =] corpus frequency of word B $- k_{11}$
\item[$k_{22}$ =] size of corpus (in tokens) - corpus frequency of A - corpus frequency of B
\item[$C_1$ =] $k_{11} + k_{12}$
\item[$C_2$ =] $k_{21} + k_{22}$
\item[$R_1$ =] $k_{11} + k_{21}$
\item[$R_2$ =] $k_{12} + k_{22}$
\item[$N$ =] $k_{11} + k_{12} + k_{21} + k_{22}$
\item[New Value = ]
    $k_{11}\log\frac{k_{11}N}{C_1R_1} +
    k_{12}\log\frac{k_{12}N}{C_1R_2} +
    k_{21}\log\frac{k_{21}N}{C_2R_1} +
    k_{22}\log\frac{k_{22}N}{C_2R_2}$
\end{description}


\subsection{Vector Similarity}

To compare to vectors (U and V), we just calculat the City Block Distance.

City Block Distance:
$$\sum_{i\in U,V} |U_i - V_i| $$

\section{Parallelizing Vector Calculations}

Using PyCUDA. %I'm not sure what to write here, since you've done all the parallelizing-ness.

\section{Results}

It went faster. :D %We hope.

%Here be a-Maze-ing graphs?

\section{Conclusions}
Everything is awesome. Especially Tesla.
%Probably need results to draw conclusions....

\section{Acknowledgments}

% TODO: how do I citations? :|
%citations go here: the source paper, mainly.
\bibliography{paper}
\bibliographystyle{plain}
\end{document}