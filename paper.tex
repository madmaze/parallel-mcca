\documentclass[12pt]{article}

\title{Parallelizing Automatic Identification of Word Translations from Unrelated Corpora}
\author{Leah Hanson, Matthias Lee}
\begin{document}
\maketitle
\begin{abstract}

Coming up with word translations based on unrelated source texts and a base
lexicon is a useful thing to do. However, it takes a long time when run on one
CPU. Parallelizing it to run on a GPU makes it run much faster.

\end{abstract}
\section{Introduction}
% Describes problem & proposed solution

It is much easier to get monolingual text than parallel text. For some
language pairs, parallel text is (nearly) non-existant. Thus, being able to
expand dictionaries based unrelated corpora can be very useful.

There is already a technique for doing this. It involves building cooccurrence
vectors based on the corpora, trimming them to the base lexicon words, and
then comparing each unknown foriegn word to the source language vectors to
find the best correspondance.

Because it involves vectors, there is a degree of obvious parallelism in the
algorithm, especially the vector comparisons at the end.

\section{Approach}
% Basic Outline
% Languages: French, German, Spanish
% Data Sources (Copuses, Base Lexicon, Test Words)
% Preprocessing
% Build Vectors
% Run tests

We are expanding dictionaries for English-Spanish, English-French, and
English-German. This means we need four monolingual corpora (one for each
language) and three dictionaries (to source the base lexicons and test words
from).

For our monolingual corpora, we used Project Gutenburg. For the dictionaries,
....we asked Chris?.

We removed all multi-word phrases from each dictionary, and then split it into
three parts. Most (what percent?) of each dictionary went for the base
lexicon. We ended up with base lexicons on the order of ??? word pairs. The
other two parts (of approximately equal size) were used as a development and a
test set.

From each corpora, we removed some common function words -- pronouns,
articles, and prepositions. We segmented the corpora into sentances and words
using the Python NLTK's default splitters. We then stemmed all the words in
each corpus and dictionary using the Python NLTK's SnowballStemmer (for the
appropriate language).

\section{Co-occurrence Counting}
% Explain Cooccurrence and why we need it
% using window size = 3; word order matters

%Context Vectors, using cooccurrence:

The goal is to end up with a set of vectors representing a corpus. Each vector
represents the context that one word (that appeared in the corpus) appears in.
The vector is composed of (word,number) mappings. Each mapping represents the
number of times the word being mapped occurs in a window around the word the
vector is describing. We used a window size of three, so each word in the
vector appeared in the three words before or the three words after the word
being described.

\section{Association Vector}
% take a context vector
% prune to base lexicon
% normalize

\section{Vector Similarity}
% seems almost too simple to get its own section...
% Equation (City Block Distance)

\section{Parallelizing Vector Calculations}

\section{Results}

\section{Conclustions}
Everything is awesome. Especially Tesla.

\section{Acknowledgments}

%citations go here

\end{document}