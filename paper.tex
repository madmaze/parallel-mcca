\documentclass[12pt]{article}

\title{Parallelizing Automatic Identification of Word Translations from Unrelated Corpora}
\author{Leah Hanson, Matthias Lee}
\begin{document}
\maketitle
\begin{abstract}

Implementation of a machine translation technique which allows for learning word translations from unrelated corpora. We offer a methods for scaling this technique to highly parallel graphics processors, using Python, NLTK and PyCUDA on NVIDA GPUs. We aim to highlight the advantages of using GPUs to speedup the vector calculation necessary for relating source words to their foriegn counterparts.

\end{abstract}
\section{Introduction}
% Describes problem & proposed solution

The exponential growth of technologies such as the Internet and Social media, it has become easier than ever to harvest enormus monolingual dataset. There has been extensive research in the field of machine translation for parallel corpora, but little attention has been paid to learning from non-parallel monolingual corpora. For the most common of languages, parallel corpora are easy to come by, but for lesser known languages it may be hard to find a large enough parallel text. Though languages rarely lack in monolingual texts. We based our approach on \cite{rapp1999automatic} and \cite{rapp1995identifying}, which soley relies on the notion that words tend to co-occur in similar frequency across different languages. Rapp cites that even very different languages such as Chinese and English share these co-occurences. We firstly aimed to replecate their results in a serial manner as a baseline and then take a step further to rework their vector calculation approach to allow for porting to graphics processors with PyCUDA. 

Rapp's approach is based on building co-occurence matrix, which keeps track of co-occurences between words appearing within a range $w$ from on another. These matricies are computed for both corpora and are then transformed into association matricies, by calculating the log-likelihood ratio for each co-occurence. These association matricies/vectors can then be compared with a simple Euclidian distance metric such as the city-block metric.

Many of these calculations are embarrassingly parallel in nature and are therefore great candidates for GPU processing.


\section{Approach}
% Basic Outline
% Languages: French, German, Spanish
% Data Sources (Copuses, Base Lexicon, Test Words)
% Preprocessing
% Build Vectors
% Run tests

We are expanding dictionaries for English-Spanish, English-French, and English-
German. This means we need four monolingual corpora (one for each language) and
three dictionaries (to source the base lexicons and test words from).

For our monolingual corpora, we used Project Gutenburg. For the dictionaries,
....we asked Chris?.

%TODO: how big were the dictionaries? how did we divide them up?

We removed all multi-word phrases from each dictionary, and then split it into
three parts. Most (what percent?) of each dictionary went for the base lexicon.
We ended up with base lexicons on the order of ??? word pairs. The other two
parts (of approximately equal size) were used as a development and a test set.

From each corpora, we removed some common function words -- pronouns, articles,
and prepositions. We segmented the corpora into sentances and words using the
Python NLTK's default splitters. We then stemmed all the words in each corpus
and dictionary using the Python NLTK's SnowballStemmer (for the appropriate
language).

After preprocessing each corpus, we build co-occurence vectors. Then we
transformed teh co-occurrence vectors into association vectors. Once we had
association vectors to represent the words in each corpus, we were ready to
attempt some translations. :)

%TODO: did we remove words that occured fewer than 100 times? at what stage?

From the target language corpora, we pulled out the association vectors
representing test words. Since each of those vectors is already in terms of
words in the base lexicon, we used the base lexicon to translate each word into
English, so that the vectors would be in the same space as their English counter
parts.

Each target langauge vector then gets compared with each English word vector,
using the City Block Distance as the comparision. For each target language word,
the English word with the closest association vector is chosen as the
translation.

\subsection{Co-occurrence Counting}

The goal is to end up with a set of vectors representing a corpus. Each vector
represents the context that one word (that appeared in the corpus) appeared in.
The vector is composed of (word,number) mappings. Each mapping represents the
number of times the word being mapped occurs in a window around the word the
vector is describing.

We used a window size of three, so each word in the
vector appeared in the three words before or the three words after the word
being described.

Rather than using the default of just putting all co-occurrences within the
window in one vector, we made position within the matter. To do this, the
final vector is created from six separate vectors: one for each position in
the window. For example, one of the six vectors would represent the words that
occur two words before the target word. In practice, we just prepended each
word with a number to represent which position it was found in, so the keys of
the dictionary differentiate between the positions.

\subsection{Association Vector}
% take a context vector
% prune to base lexicon
% convert values
% normalize

Turning a co-occurrence vector into an association vector is actually fairly
straight-forward. For each vector, remove all words that are not in the base
lexicon. At this point, use a fancy formula to convert each raw count into an
association value.  Then, normalize the values so that each vector sums to
one.

To convert a raw co-occurrence count into an association value, you need: the
count, the total number of times the word this context vector represents was
seen, the number of times this word was seen, and the total number of word
tokens in the corpus.

\begin{description}
\item[$k_{11}$ =] frequency of common occurrence of word A and word B
\item[$k_{12}$ =] corpus frequency of word A $- k_{11}$
\item[$k_{21}$ =] corpus frequency of word B $- k_{11}$
\item[$k_{22}$ =] size of corpus (in tokens) - corpus frequency of A - corpus frequency of B
\item[$C_1$ =] $k_{11} + k_{12}$
\item[$C_2$ =] $k_{21} + k_{22}$
\item[$R_1$ =] $k_{11} + k_{21}$
\item[$R_2$ =] $k_{12} + k_{22}$
\item[$N$ =] $k_{11} + k_{12} + k_{21} + k_{22}$
\item[New Value = ]
    $k_{11}\log\frac{k_{11}N}{C_1R_1} +
    k_{12}\log\frac{k_{12}N}{C_1R_2} +
    k_{21}\log\frac{k_{21}N}{C_2R_1} +
    k_{22}\log\frac{k_{22}N}{C_2R_2}$
\end{description}


\subsection{Vector Similarity}

To compare to vectors (U and V), we just calculat the City Block Distance.

City Block Distance:
$$\sum_{i\in U,V} |U_i - V_i| $$

\section{Parallelizing Vector Calculations}

Using PyCUDA. %I'm not sure what to write here, since you've done all the parallelizing-ness.

\section{Results}

It went faster. :D %We hope.

%Here be a-Maze-ing graphs?

\section{Conclusions}
Everything is awesome. Especially Tesla.
%Probably need results to draw conclusions....

\section{Acknowledgments}

% TODO: how do I citations? :|
%citations go here: the source paper, mainly.
\bibliography{paper}
\bibliographystyle{plain}
\end{document}