\documentclass[12pt]{article}

\title{Parallelizing Automatic Identification of Word Translations from Unrelated Corpora}
\author{Leah Hanson, Matthias Lee}
\begin{document}
\maketitle
\begin{abstract}

Coming up with word translations based on unrelated source texts and a base
lexicon is a useful thing to do. However, it takes a long time when run on one
CPU. Parallelizing it to run on a GPU makes it run much faster.

\end{abstract}
\section{Introduction}
% Describes problem & proposed solution

It is much easier to get monolingual text than parallel text. For some
language pairs, parallel text is (nearly) non-existant. Thus, being able to
expand dictionaries based unrelated corpora can be very useful.

There is already a technique for doing this. It involves building cooccurrence
vectors based on the corpora, trimming them to the base lexicon words, and
then comparing each unknown foriegn word to the source language vectors to
find the best correspondance.

Because it involves vectors, there is a degree of obvious parallelism in the
algorithm, especially the vector comparisons at the end.

\section{Approach}
% Basic Outline
% Languages: French, German, Spanish
% Data Sources (Copuses, Base Lexicon, Test Words)
% Preprocessing
% Build Vectors
% Run tests

We are expanding dictionaries for English-Spanish, English-French, and English-
German. This means we need four monolingual corpora (one for each language) and
three dictionaries (to source the base lexicons and test words from).

For our monolingual corpora, we used Project Gutenburg. For the dictionaries,
....we asked Chris?.

%TODO: how big were the dictionaries? how did we divide them up?

We removed all multi-word phrases from each dictionary, and then split it into
three parts. Most (what percent?) of each dictionary went for the base lexicon.
We ended up with base lexicons on the order of ??? word pairs. The other two
parts (of approximately equal size) were used as a development and a test set.

From each corpora, we removed some common function words -- pronouns, articles,
and prepositions. We segmented the corpora into sentances and words using the
Python NLTK's default splitters. We then stemmed all the words in each corpus
and dictionary using the Python NLTK's SnowballStemmer (for the appropriate
language).

After preprocessing each corpus, we build co-occurence vectors. Then we
transformed teh co-occurrence vectors into association vectors. Once we had
association vectors to represent the words in each corpus, we were ready to
attempt some translations. :)

%TODO: did we remove words that occured fewer than 100 times? at what stage?

From the target language corpora, we pulled out the association vectors
representing test words. Since each of those vectors is already in terms of
words in the base lexicon, we used the base lexicon to translate each word into
English, so that the vectors would be in the same space as their English counter
parts.

Each target langauge vector then gets compared with each English word vector,
using the City Block Distance as the comparision. For each target language word,
the English word with the closest association vector is chosen as the
translation.

\subsection{Co-occurrence Counting}

The goal is to end up with a set of vectors representing a corpus. Each vector
represents the context that one word (that appeared in the corpus) appeared in.
The vector is composed of (word,number) mappings. Each mapping represents the
number of times the word being mapped occurs in a window around the word the
vector is describing.

We used a window size of three, so each word in the
vector appeared in the three words before or the three words after the word
being described.

Rather than using the default of just putting all co-occurrences within the
window in one vector, we made position within the matter. To do this, the
final vector is created from six separate vectors: one for each position in
the window. For example, one of the six vectors would represent the words that
occur two words before the target word. In practice, we just prepended each
word with a number to represent which position it was found in, so the keys of
the dictionary differentiate between the positions.

\subsection{Association Vector}
% take a context vector
% prune to base lexicon
% convert values
% normalize

Turning a co-occurrence vector into an association vector is actually fairly
straight-forward. For each vector, remove all words that are not in the base
lexicon. At this point, use a fancy formula to convert each raw count into an
association value.  Then, normalize the values so that each vector sums to
one.

To convert a raw co-occurrence count into an association value, you need: the
count, the total number of times the word this context vector represents was
seen, the number of times this word was seen, and the total number of word
tokens in the corpus.

\begin{description}
\item[$k_{11}$ =] frequency of common occurrence of word A and word B
\item[$k_{12}$ =] corpus frequency of word A $- k_{11}$
\item[$k_{21}$ =] corpus frequency of word B $- k_{11}$
\item[$k_{22}$ =] size of corpus (in tokens) - corpus frequency of A - corpus frequency of B
\item[$C_1$ =] $k_{11} + k_{12}$
\item[$C_2$ =] $k_{21} + k_{22}$
\item[$R_1$ =] $k_{11} + k_{21}$
\item[$R_2$ =] $k_{12} + k_{22}$
\item[$N$ =] $k_{11} + k_{12} + k_{21} + k_{22}$
\item[New Value = ]
    $k_{11}\log\frac{k_{11}N}{C_1R_1} +
    k_{12}\log\frac{k_{12}N}{C_1R_2} +
    k_{21}\log\frac{k_{21}N}{C_2R_1} +
    k_{22}\log\frac{k_{22}N}{C_2R_2}$
\end{description}


\subsection{Vector Similarity}

To compare to vectors (U and V), we just calculat the City Block Distance.

City Block Distance:
$$\sum_{i\in U,V} |U_i - V_i| $$

\section{Parallelizing Vector Calculations}

Using PyCUDA. %I'm not sure what to write here, since you've done all the parallelizing-ness.

\section{Results}

It went faster. :D %We hope.

%Here be a-Maze-ing graphs?

\section{Conclusions}
Everything is awesome. Especially Tesla.
%Probably need results to draw conclusions....

\section{Acknowledgments}

% TODO: how do I citations? :|
%citations go here: the source paper, mainly.

\end{document}